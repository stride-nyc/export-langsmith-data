{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangSmith Trace Performance Analysis\n",
    "\n",
    "This notebook analyzes LangSmith trace exports to understand:\n",
    "1. **Latency Distribution**: p50/p95/p99 metrics, outlier detection\n",
    "2. **Bottleneck Identification**: Which nodes consume the most time\n",
    "3. **Parallel Execution Verification**: Do validators run in parallel?\n",
    "\n",
    "**Input**: JSON export file from `export_langsmith_traces.py`  \n",
    "**Output**: CSV files with analysis results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from analyze_traces import (\n",
    "    load_from_json,\n",
    "    analyze_latency_distribution,\n",
    "    identify_bottlenecks,\n",
    "    verify_parallel_execution,\n",
    ")\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Trace Data\n",
    "\n",
    "Load the exported JSON file. Update the file path below to point to your export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Update this path to your export file\nexport_file = \"../sample_traces_export.json\"\n\nprint(f\"Loading trace data from: {export_file}\")\ndataset = load_from_json(export_file)\n\nprint(\"\\nLoaded:\")\nprint(f\"  - Workflows: {len(dataset.workflows)}\")\nprint(f\"  - Orphan traces: {len(dataset.orphan_traces)}\")\nprint(f\"  - Hierarchical data: {dataset.is_hierarchical}\")\n\nif dataset.workflows:\n    print(\"\\nSample workflow:\")\n    sample = dataset.workflows[0]\n    print(f\"  - Root: {sample.root_trace.name}\")\n    print(f\"  - Duration: {sample.total_duration/60:.1f} minutes\")\n    print(f\"  - Nodes: {list(sample.nodes.keys())}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Latency Distribution Analysis\n",
    "\n",
    "Calculate percentile metrics and identify outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latency_dist = analyze_latency_distribution(dataset.workflows)\n",
    "\n",
    "print(\"Latency Distribution Results:\")\n",
    "print(f\"  p50 (median): {latency_dist.p50_minutes:.1f} minutes\")\n",
    "print(f\"  p95: {latency_dist.p95_minutes:.1f} minutes\")\n",
    "print(f\"  p99: {latency_dist.p99_minutes:.1f} minutes\")\n",
    "print(f\"  Range: {latency_dist.min_minutes:.1f} - {latency_dist.max_minutes:.1f} minutes\")\n",
    "print(f\"  Mean ± StdDev: {latency_dist.mean_minutes:.1f} ± {latency_dist.std_dev_minutes:.1f} minutes\")\n",
    "print(\"\\nOutliers:\")\n",
    "print(f\"  Above 23 min: {len(latency_dist.outliers_above_23min)} workflows\")\n",
    "print(f\"  Below 7 min: {len(latency_dist.outliers_below_7min)} workflows\")\n",
    "print(\"\\nClaim Validation:\")\n",
    "print(f\"  % within 7-23 min range: {latency_dist.percent_within_7_23_claim:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bottleneck Identification\n",
    "\n",
    "Identify which nodes consume the most time across workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck_analysis = identify_bottlenecks(dataset.workflows)\n",
    "\n",
    "print(\"Bottleneck Analysis Results:\")\n",
    "print(f\"  Primary bottleneck: {bottleneck_analysis.primary_bottleneck}\")\n",
    "print(f\"  Top 3 bottlenecks: {', '.join(bottleneck_analysis.top_3_bottlenecks)}\")\n",
    "print(\"\\nNode Performance Details:\")\n",
    "print(f\"{'Node Name':<30} {'Exec Count':<12} {'Avg Duration':<15} {'% of Workflow':<15}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for node in bottleneck_analysis.node_performances[:10]:  # Top 10\n",
    "    print(\n",
    "        f\"{node.node_name:<30} {node.execution_count:<12} \"\n",
    "        f\"{node.avg_duration_seconds:>8.1f}s {node.avg_percent_of_workflow:>13.1f}%\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Parallel Execution Verification\n",
    "\n",
    "Verify if validator nodes execute in parallel and calculate time savings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_evidence = verify_parallel_execution(dataset.workflows)\n",
    "\n",
    "print(\"Parallel Execution Verification Results:\")\n",
    "print(f\"  Verdict: {'PARALLEL' if parallel_evidence.is_parallel else 'SEQUENTIAL'}\")\n",
    "print(f\"  Confidence: {parallel_evidence.confidence.upper()}\")\n",
    "print(\"\\nWorkflow Counts:\")\n",
    "print(f\"  Parallel workflows: {parallel_evidence.parallel_confirmed_count}\")\n",
    "print(f\"  Sequential workflows: {parallel_evidence.sequential_count}\")\n",
    "print(\"\\nTiming Metrics:\")\n",
    "print(f\"  Avg start time delta: {parallel_evidence.avg_start_time_delta_seconds:.1f}s\")\n",
    "print(f\"  Avg sequential time: {parallel_evidence.avg_sequential_time_seconds:.1f}s\")\n",
    "print(f\"  Avg parallel time: {parallel_evidence.avg_parallel_time_seconds:.1f}s\")\n",
    "print(f\"  Avg time savings: {parallel_evidence.avg_time_savings_seconds:.1f}s ({parallel_evidence.avg_time_savings_seconds/60:.1f} min)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export Results to CSV\n",
    "\n",
    "Save analysis results to CSV files for further analysis or reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"../output\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Export latency distribution\n",
    "latency_csv_path = output_dir / \"latency_distribution.csv\"\n",
    "with open(latency_csv_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(latency_dist.to_csv())\n",
    "print(f\"✓ Exported latency distribution to: {latency_csv_path}\")\n",
    "\n",
    "# Export bottleneck analysis\n",
    "bottleneck_csv_path = output_dir / \"bottleneck_analysis.csv\"\n",
    "with open(bottleneck_csv_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(bottleneck_analysis.to_csv())\n",
    "print(f\"✓ Exported bottleneck analysis to: {bottleneck_csv_path}\")\n",
    "\n",
    "# Export parallel execution evidence\n",
    "parallel_csv_path = output_dir / \"parallel_execution_analysis.csv\"\n",
    "with open(parallel_csv_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(parallel_evidence.to_csv())\n",
    "print(f\"✓ Exported parallel execution analysis to: {parallel_csv_path}\")\n",
    "\n",
    "print(f\"\\n✓ All results exported to: {output_dir.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Performance analysis complete! Check the `output/` directory for CSV files with detailed results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}